---
---
@article{park2024Snapbot,
      author={Park, Sunghyun and Chai, Yoonbyung and Ka, Seungyup and Kim, Hyeonseong and Park, Sangbeom and Gim, Kevin and Kim, Joohyung and Choi, Sungjoon},
      title={Learning Latent Prior for Rapid Adaptation of Legged Robots to Unexpected Amputation},
      journal={IEEE International Conference on Robotics and Automation (ICRA), Submitted},
      publisher={IEEE},
      year={2024},
      preview={snapbot.png},
      selected={true},
}

@article{park2024QD-teleop,
      author={Park, Sangbeom and Yoon, Taerim and Lee, Joonhyung and Park, Sunghyun and Choi, Sungjoon},
      title={Quality-Diversity based Semi-Autonomous Teleoperation using Reinforcement Learning},
      journal={Neural Networks (Submitted)},
      publisher={Elsevier},
      year={2024},
      preview={QD-teleop.png},
      pdf={https://pdf.sciencedirectassets.com/271125/1-s2.0-S0893608024X00088/1-s2.0-S0893608024004672/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGkaCXVzLWVhc3QtMSJHMEUCIHRaBelP1JguyCdpNvhhLdJ3/N4l6XEpFSqigYOM3ifTAiEA31o0Dvv/jE6w2+2T70NKowlx6eu0mttDwaRigriKDT0qvAUIof//////////ARAFGgwwNTkwMDM1NDY4NjUiDOXF3V3L68ZuGvewASqQBUuMXqNQi4CZb7U9+NT6T7dTaFlwY+ezlguRK44wCKwOmaYk49USg0HhMs16LsmtVmJEeHrJTnmcECtS6UpjQp33FufDu58Ki6pGlIROAgZqZfX947Fs3iBl96Iifh7Ls+HGfDMmOeHKc1vtiTJpth3XgCoZYZWEYRWa8vfhYmDA0cZmAkxFtj41i3YyDGPhYybwEPOFYldOkgci3YidJAAT8a8m+yOwxPr0SiTcwC/jCq7f7ehVHI4UuKxPape2Oxrl4EdyNOwRHCXq6thjDgsEbcAAxz044WRzMaco/OcddH8MTgYscRokAZB2/26sHfjF57Ksg82yw34S1yT/PcYErNtujRdtfdYMo7z9T/6n2duXNDFxK/BmgmvVRM9b9tbbDOv3vWapm0Tg6Kx79+9Q66T5w2pnbq2SaROXXtcSp/GdOXnekAwa+OD40ZjhDcCKXuIf8bovFKkjgCbTQsUsaVCIcEnwPYzOfGq7v/osdJb2XUd1Vq249YVJ9B11G0yeYSl2gZuDEdNZzt2/sGI9/qHqIUywtJpuJ8JC94oyCps0ncLUktVumrA8SWo9nKOJqNBNUx9adblVdB6wlTmkEqhIYYeUq458O/RlE+igYe9vcSC3zrLvZ/L+VLiwMvEEWTFrXEL7DwGwTiRDDAqzGyVphjg5lX2zJKOh2d3Z3BecDWUMt8c4rOyDNqa7tn7rtrc+t2gzSjB59nkOzBrmtwlzNMvhrBNhJUUWRBuDHC2YT52G26qIQMce94R2ScxRhUm8vZEAHyJS/3MMqzrljwTj+f7yNSbikA3NC1IjaLI0jFYHjNW9bIhqsgI74/axCptIXD5vDcVd+3LFtVszqXO03eTC1BZcc0/Yq3KIMKWhv7cGOrEBev8YfO5fmi3WNTfRUg129Z8PU1oVDHr6JVd78LYB14jFy3fwssVfuG8LV2unvUMiPTNaokLFa4BDH7VpJkKi280At41RQWwupKRnuVBj6DOqX8x1TxoqLxWU6we30uMyyOxCBkUILh83KeAVbZfuBLWwH9dZtWKskr1n/nUsTjRARrrf36+KoyYcfQjIMkSWYTvPCeosfJa9/stH7bfAsP6Bw4Uhj/a1CJHK0CeTlsCW&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240922T091543Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6PREFFSN/20240922/us-east-1/s3/aws4_request&X-Amz-Signature=09e3ccddd1380c8f5b00933aeff4f859be8f6ac80a19422287e25fddae3777a8&hash=e42566bd793306c7c701bca4b4c6242b054eb4cbee5f41350c6b4933251ed152&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0893608024004672&tid=spdf-6d28be69-a1d2-4a66-9350-508d6fbd58e4&sid=9144c1de8c00d54cbb5bbd569f764f847ceagxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=11175f025153505552565f&rr=8c7131280bbdaa35&cc=kr},
      website={https://park-sangbeom.github.io/smore/},
      selected={true},
}

@article{lee2024vpi,
      author={Lee, Joonhyung, and Park, Sangbeom and Kwon, Yongin and Lee, Jemin and Ahn, Minwook and Choi, Sungjoon},
      title={Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation},
      journal={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
      publisher={IEEE},
      year={2024},
      abstract={In robotic object manipulation, human preferences can often be influenced by the visual attributes of objects, such as color and shape. These properties play a crucial role in operating a robot to interact with objects and align with human intention. In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw visual observations in tabletop manipulation environments with a variety of object types, named Visual Preference Inference (VPI). To facilitate visual reasoning in the context of manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR employs a prompting mechanism that describes the difference between the consecutive images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user's preference. This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual environment during manipulation tasks. Furthermore, we incorporate such texts along with a sequence of images to infer the user's preferences. Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in both simulation and real-world environments. Code and videos are available at: \href{https://joonhyung-lee.github.io/vpi/}{https://joonhyung-lee.github.io/vpi/}},
      pdf={https://arxiv.org/abs/2403.11513},
      code={https://github.com/joonhyung-lee/vpi},
      website={https://joonhyung-lee.github.io/vpi/},
      preview={vpi.gif},
      selected={true},
}

@article{lee2023spots,
      title={SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems}, 
      author={Lee, Joonhyung and Park, Sangbeom and Park, Jeongeun and Lee, Kyungjae and Choi, Sungjoon},
      journal={IEEE International Conference on Robotics and Automation (ICRA)},
      publisher={IEEE},
      year={2024},
      abstract={Pick-and-place is one of the fundamental tasks in robotics research. However, the attention has been mostly focused on the ``pick'' task, leaving the ``place'' task relatively unexplored. In this paper, we address the problem of placing objects in the context of a teleoperation framework. Particularly, we focus on two aspects of the place task: stability robustness and contextual reasonableness of object placements. Our proposed method combines simulation-driven physical stability verification via real-to-sim and the semantic reasoning capability of large language models. In other words, given place context information (e.g., user preferences, object to place, and current scene information), our proposed method outputs a probability distribution over the possible placement candidates, considering the robustness and reasonableness of the place task. Our proposed method is extensively evaluated in two simulation and one real world environments and we show that our method can greatly increase the physical plausibility of the placement as well as contextual soundness while considering user preferences.},
      pdf={https://arxiv.org/pdf/2309.13937.pdf},
      code={https://github.com/joonhyung-lee/spots},
      website={https://joonhyung-lee.github.io/spots/},
      preview={spots.gif},
      selected={true}
}

@article{park2023clara,
      title={CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents}, 
      author={Park, Jeongeun and Lim, Seungwon and Lee, Joonhyung and Park, Sangbeom and Chang, Minsuk and Yu, Youngjae and Choi, Sungjoon},
      journal={IEEE Robotics and Automation Letters (RA-L)},
      publisher={IEEE},
      year={2023},
      abstract={In this paper, we focus on inferring whether the given user command is clear, ambiguous, or infeasible in the context of interactive robotic agents utilizing large language models (LLMs). To tackle this problem, we first present an uncertainty estimation method for LLMs to classify whether the command is certain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the command is classified as uncertain, we further distinguish it between ambiguous or infeasible commands leveraging LLMs with situational aware context in a zero-shot manner. For ambiguous commands, we disambiguate the command by interacting with users via question generation with LLMs. We believe that proper recognition of the given commands could lead to a decrease in malfunction and undesired actions of the robot, enhancing the reliability of interactive robot agents. We present a dataset for robotic situational awareness, consisting pair of high-level commands, scene descriptions, and labels of command type (i.e., clear, ambiguous, or infeasible). We validate the proposed method on the collected dataset, pick-and-place tabletop simulation. Finally, we demonstrate the proposed approach in real-world human-robot interaction experiments, i.e., handover scenarios.},
      pdf={https://arxiv.org/abs/2306.10376},
      code={https://github.com/jeongeun980906/CLARA-SaGC-Code},
      website={https://clararobot.github.io/},
      abbr={RA-L},
      preview={clara.gif},
      selected={true}
}

@article{park2022semiteleop,
      author={Park, Sangbeom and Chai, Yoonbyung and Park, Sunghyun and Park, Jeongeun and Lee, Kyungjae and Choi, Sungjoon},
      title={Semi-Autonomous Teleoperation via Learning Non-Prehensile Manipulation Skills},
      journal={IEEE International Conference on Robotics and Automation (ICRA)},
      publisher={IEEE},
      year={2022},
      archivePrefix={arXiv},
      abstract={In this paper, we present a semi-autonomous teleoperation framework for a pick-and-place task using an RGB-D sensor. In particular, we assume that the target object is located in a cluttered environment where both prehensile grasping and non-prehensile manipulation are combined for efficient teleoperation. A trajectory-based reinforcement learning is utilized for learning the non-prehensile manipulation to rearrange the objects for enabling direct grasping. From the depth image of the cluttered environment and the location of the goal object, the learned policy can provide multiple options of non-prehensile manipulation to the human operator. We carefully design a reward function for the rearranging task where the policy is trained in a simulational environment. Then, the trained policy is transferred to a real-world and evaluated in a number of real-world experiments with the varying number of objects where we show that the proposed method outperforms manual keyboard control in terms of the time duration for the grasping.},
      pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811823&casa_token=mABguuXoPK8AAAAA:SOwXSqtMBL-cnRkkL57we6mWNDn5Gvu15AEOF0ZNp2wuiI2Br398nUK_aoQcc8HfF4u_m0CrTA&tag=1},
      website={https://joonhyung-lee.github.io/robust-detection-for-elevator-boarding/},
      preview={semi-teleop.gif},
      selected={true}
}
